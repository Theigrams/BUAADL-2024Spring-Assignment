{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def load_dict(dict_path):\n",
    "    vocab = {}\n",
    "    i = 0\n",
    "    with open(dict_path, \"r\", encoding=\"utf-8\") as fin:\n",
    "        for line in fin:\n",
    "            key = line.strip(\"\\n\")\n",
    "            vocab[key] = i\n",
    "            i += 1\n",
    "    return vocab\n",
    "\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self, vocab_path, oov_token=\"OOV\"):\n",
    "        self.word2id = load_dict(vocab_path)\n",
    "        self.id2word = {v: k for k, v in self.word2id.items()}\n",
    "        self.vocab_size = len(self.word2id)\n",
    "        # out-of-vocabulary token\n",
    "        self.oov_token = oov_token\n",
    "\n",
    "    def word_to_id(self, word):\n",
    "        # TODO: 将 word 转换为其对应的 ID, 如果 word 不在词典中, 则返回 oov_token 对应的ID\n",
    "        return self.word2id.get(word, self.word2id[self.oov_token])\n",
    "\n",
    "    def id_to_word(self, id):\n",
    "        return self.id2word[id]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2id)\n",
    "\n",
    "    def __call__(self, words):\n",
    "        return [self.word_to_id(word) for word in words]\n",
    "\n",
    "\n",
    "class NerDataset(Dataset):\n",
    "    def __init__(self, data_path, word_vocab: Vocab, label_vocab: Vocab):\n",
    "        self.word_vocab = word_vocab\n",
    "        self.label_vocab = label_vocab\n",
    "        self.word_ids = []\n",
    "        self.label_ids = []\n",
    "        self.read_data(data_path)\n",
    "\n",
    "    def read_data(self, path):\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as fp:\n",
    "            next(fp)  # Skip the header line\n",
    "            for line in fp.readlines():\n",
    "                words, labels = line.strip(\"\\n\").split(\"\\t\")\n",
    "                words = words.split(\"\\002\")\n",
    "                labels = labels.split(\"\\002\")\n",
    "\n",
    "                # TODO: 使用 word_vocab 将 words 转换为 word_id, 类型为 torch.long\n",
    "                word_id = torch.tensor(self.word_vocab(words), dtype=torch.long)\n",
    "                # TODO: 使用 label_vocab 将 labels 转换为 label_id, 类型为 torch.long\n",
    "                label_id = torch.tensor(self.label_vocab(labels), dtype=torch.long)\n",
    "\n",
    "                self.word_ids.append(word_id)\n",
    "                self.label_ids.append(label_id)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word_ids)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.word_ids[item], self.label_ids[item], len(self.word_ids[item])\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    words, labels, seqlens = zip(*batch)\n",
    "    words = nn.utils.rnn.pad_sequence(words, batch_first=True, padding_value=20939)  # word_vocab([\"OOV\"])\n",
    "    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=12)  # label_vocab([\"O\"])\n",
    "    seqlens = torch.tensor(seqlens, dtype=torch.long)\n",
    "    return words, labels, seqlens\n",
    "\n",
    "\n",
    "def load_data(data_folder=\"data\", batch_size=32):\n",
    "    path = Path(data_folder)\n",
    "    word_vocab = Vocab(path / \"word.dic\", \"OOV\")\n",
    "    label_vocab = Vocab(path / \"tag.dic\", \"O\")\n",
    "    train_ds = NerDataset(path / \"train.txt\", word_vocab, label_vocab)\n",
    "    test_ds = NerDataset(path / \"test.txt\", word_vocab, label_vocab)\n",
    "    train_dl = DataLoader(\n",
    "        train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, num_workers=4, pin_memory=True\n",
    "    )\n",
    "    test_dl = DataLoader(\n",
    "        test_ds, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, num_workers=4, pin_memory=True\n",
    "    )\n",
    "\n",
    "    return train_dl, test_dl, word_vocab, label_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要安装 torchcrf 库\n",
    "\n",
    "```bash\n",
    "!pip install pytorch-crf\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchcrf import CRF\n",
    "\n",
    "\n",
    "class BiGRUWithCRF(nn.Module):\n",
    "    def __init__(self, embedding_dim=768, hidden_size=256, word_vocab_len=20940, label_vocab_len=13):\n",
    "        super(BiGRUWithCRF, self).__init__()\n",
    "\n",
    "        self.word_emb = nn.Embedding(word_vocab_len, embedding_dim)\n",
    "\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_size, num_layers=2, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, label_vocab_len)\n",
    "        self.crf = CRF(label_vocab_len, batch_first=True)\n",
    "\n",
    "    def _get_features(self, x):\n",
    "        embs = self.word_emb(x)\n",
    "        enc, _ = self.gru(embs)\n",
    "        feats = self.fc(enc)\n",
    "        return feats\n",
    "\n",
    "    def forward(self, x, y=None, lens=None, is_test=False):\n",
    "        emissions = self._get_features(x)\n",
    "        if lens is None:\n",
    "            mask = None\n",
    "        else:\n",
    "            mask = torch.arange(emissions.shape[1]).expand(len(lens), emissions.shape[1]).to(lens.device) < lens.view(\n",
    "                -1, 1\n",
    "            )\n",
    "        if not is_test:  # 训练阶段，返回loss\n",
    "            loss = -self.crf.forward(emissions, y, mask, reduction=\"mean\")\n",
    "            return loss\n",
    "        else:  # 测试阶段，返回decoding结果\n",
    "            decode = self.crf.decode(emissions, mask)\n",
    "            return decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accumulator:\n",
    "    def __init__(self, n):\n",
    "        self.data = [0.0] * n\n",
    "\n",
    "    def add(self, *args):\n",
    "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
    "\n",
    "    def reset(self):\n",
    "        self.data = [0.0] * len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "def train_epoch(net, train_iter, loss_fn, optimizer):\n",
    "    net.train()\n",
    "    device = next(net.parameters()).device\n",
    "    metrics = Accumulator(2)\n",
    "    for X, y, lens in train_iter:\n",
    "        X, y, lens = X.to(device), y.to(device), lens.to(device)\n",
    "        loss = net(X, y, lens)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        num = y.shape[0] * y.shape[1]\n",
    "        metrics.add(loss * num, num)\n",
    "        train_loss = metrics[0] / metrics[1]\n",
    "    return train_loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_model(net, test_iter, loss_fn):\n",
    "    net.eval()\n",
    "    device = next(net.parameters()).device\n",
    "    metrics = Accumulator(3)\n",
    "    for X, y, lens in test_iter:\n",
    "        X, y, lens = X.to(device), y.to(device), lens.to(device)\n",
    "        y_hat = net(X, is_test=True)\n",
    "\n",
    "        num = y.shape[0] * y.shape[1]\n",
    "        metrics.add(accuracy(y_hat, y, lens) * num, num)\n",
    "    test_acc = metrics[0] / metrics[1]\n",
    "    return test_acc\n",
    "\n",
    "\n",
    "def accuracy(y_hat, y_true, lens):\n",
    "    count = 0\n",
    "    for i in range(len(y_hat)):\n",
    "        y_hat_clip = torch.tensor(y_hat[i][: lens[i]], dtype=torch.long, device=y_true.device)\n",
    "        y_true_clip = y_true[i][: lens[i]]\n",
    "        count += torch.sum(y_hat_clip == y_true_clip).item()\n",
    "    return count / torch.sum(lens).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Train Loss: 16.249954 - Test Acc: 0.978001\n",
      "Epoch 2/50 - Train Loss: 1.494200 - Test Acc: 0.990030\n",
      "Epoch 3/50 - Train Loss: 0.498673 - Test Acc: 0.994126\n",
      "Epoch 4/50 - Train Loss: 0.212329 - Test Acc: 0.993866\n",
      "Epoch 5/50 - Train Loss: 0.129001 - Test Acc: 0.993512\n",
      "Epoch 6/50 - Train Loss: 0.076460 - Test Acc: 0.994980\n",
      "Epoch 7/50 - Train Loss: 0.046636 - Test Acc: 0.994973\n",
      "Epoch 8/50 - Train Loss: 0.019184 - Test Acc: 0.995385\n",
      "Epoch 9/50 - Train Loss: 0.013090 - Test Acc: 0.994816\n",
      "Epoch 10/50 - Train Loss: 0.010310 - Test Acc: 0.994971\n",
      "Epoch 11/50 - Train Loss: 0.007894 - Test Acc: 0.994396\n",
      "Epoch 12/50 - Train Loss: 0.006336 - Test Acc: 0.994832\n",
      "Epoch 13/50 - Train Loss: 0.005289 - Test Acc: 0.994971\n",
      "Epoch 14/50 - Train Loss: 0.004713 - Test Acc: 0.994816\n",
      "Epoch 15/50 - Train Loss: 0.003998 - Test Acc: 0.994669\n",
      "Epoch 16/50 - Train Loss: 0.003703 - Test Acc: 0.994963\n",
      "Epoch 17/50 - Train Loss: 0.005941 - Test Acc: 0.994693\n",
      "Epoch 18/50 - Train Loss: 0.016039 - Test Acc: 0.994689\n",
      "Epoch 19/50 - Train Loss: 0.012210 - Test Acc: 0.995416\n",
      "Epoch 20/50 - Train Loss: 0.004682 - Test Acc: 0.995112\n",
      "Epoch 21/50 - Train Loss: 0.003615 - Test Acc: 0.995102\n",
      "Epoch 22/50 - Train Loss: 0.002590 - Test Acc: 0.995410\n",
      "Epoch 23/50 - Train Loss: 0.002422 - Test Acc: 0.995410\n",
      "Epoch 24/50 - Train Loss: 0.002235 - Test Acc: 0.995256\n",
      "Epoch 25/50 - Train Loss: 0.002212 - Test Acc: 0.995410\n",
      "Epoch 26/50 - Train Loss: 0.002186 - Test Acc: 0.995734\n",
      "Epoch 27/50 - Train Loss: 0.001958 - Test Acc: 0.995256\n",
      "Epoch 28/50 - Train Loss: 0.001919 - Test Acc: 0.995117\n",
      "Epoch 29/50 - Train Loss: 0.002035 - Test Acc: 0.995117\n",
      "Epoch 30/50 - Train Loss: 0.001837 - Test Acc: 0.995117\n",
      "Epoch 31/50 - Train Loss: 0.001774 - Test Acc: 0.995117\n",
      "Epoch 32/50 - Train Loss: 0.001800 - Test Acc: 0.995117\n",
      "Epoch 33/50 - Train Loss: 0.001769 - Test Acc: 0.994963\n",
      "Epoch 34/50 - Train Loss: 0.001786 - Test Acc: 0.994963\n",
      "Epoch 35/50 - Train Loss: 0.001788 - Test Acc: 0.994963\n",
      "Epoch 36/50 - Train Loss: 0.001741 - Test Acc: 0.994963\n",
      "Epoch 37/50 - Train Loss: 0.001755 - Test Acc: 0.994963\n",
      "Epoch 38/50 - Train Loss: 0.001674 - Test Acc: 0.994963\n",
      "Epoch 39/50 - Train Loss: 0.001666 - Test Acc: 0.994963\n",
      "Epoch 40/50 - Train Loss: 0.001633 - Test Acc: 0.995100\n",
      "Epoch 41/50 - Train Loss: 0.001602 - Test Acc: 0.995100\n",
      "Epoch 42/50 - Train Loss: 0.001542 - Test Acc: 0.994946\n",
      "Epoch 43/50 - Train Loss: 0.001536 - Test Acc: 0.995100\n",
      "Epoch 44/50 - Train Loss: 0.001501 - Test Acc: 0.995100\n",
      "Epoch 45/50 - Train Loss: 0.001657 - Test Acc: 0.995123\n",
      "Epoch 46/50 - Train Loss: 0.001415 - Test Acc: 0.995116\n",
      "Epoch 47/50 - Train Loss: 0.001374 - Test Acc: 0.995116\n",
      "Epoch 48/50 - Train Loss: 0.001351 - Test Acc: 0.995116\n",
      "Epoch 49/50 - Train Loss: 0.001309 - Test Acc: 0.995116\n",
      "Epoch 50/50 - Train Loss: 0.001310 - Test Acc: 0.995123\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BiGRUWithCRF(300, 300, 20940, 13)\n",
    "model = model.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "train_iter, test_iter, word_vocab, label_vocab = load_data(batch_size=32)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "train_ls, test_ls, train_acc_ls, test_acc_ls = [], [], [], []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss = train_epoch(model, train_iter, loss_fn, optimizer)\n",
    "    test_acc = eval_model(model, test_iter, loss_fn)\n",
    "    print(f\"Epoch {epoch}/{epochs} - Train Loss: {train_loss:.6f} - Test Acc: {test_acc:.6f}\")\n",
    "    train_ls.append(train_loss)\n",
    "    test_acc_ls.append(test_acc)\n",
    "# save model\n",
    "torch.save(model.state_dict(), \"model_crf.pth\")\n",
    "torch.save(model.crf.state_dict(), \"crf.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BiGRUWithCRF(300, 300, 20940, 13).to(device)\n",
    "model.load_state_dict(torch.load(\"model_crf.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_information(text, tags):\n",
    "    extracted_info = {\"P\": \"\", \"T\": \"\", \"A1\": \"\", \"A2\": \"\", \"A3\": \"\", \"A4\": \"\"}\n",
    "    tag_to_chinese = {\"P\": \"姓名\", \"T\": \"电话\", \"A1\": \"省份\", \"A2\": \"城市\", \"A3\": \"县区\", \"A4\": \"详细地址\"}\n",
    "\n",
    "    for char, tag in zip(text, tags):\n",
    "        tag_key = tag.split(\"-\")[0]  # 获取标签的主要部分（例如，从P-B获取P）\n",
    "        if tag_key in extracted_info:\n",
    "            extracted_info[tag_key] += \"\".join(char)\n",
    "    info = {tag_to_chinese[k]: v for k, v in extracted_info.items()}\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'姓名': '刘伟', '电话': '18600009172', '省份': '', '城市': '北京市', '县区': '昌平区', '详细地址': '高教园南三街9号北京航空航天大学'}\n"
     ]
    }
   ],
   "source": [
    "text = \"北京市昌平区高教园南三街9号北京航空航天大学18600009172刘伟\"\n",
    "text_ids = torch.tensor(word_vocab(text), dtype=torch.long).to(device)\n",
    "y_crf = model(text_ids.unsqueeze(0), is_test=True)\n",
    "tags_pred = [label_vocab.id2word[int(x)] for x in y_crf[0]]\n",
    "info = extract_information(text, tags_pred)\n",
    "print(info)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
